{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/leiluk1/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import gutenberg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig's solution: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text): \n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "CORPUS_FILE = 'data/big.txt'\n",
        "words_dict = Counter(words(open(CORPUS_FILE).read()))\n",
        "N = sum(words_dict.values())\n",
        "\n",
        "def P(word): \n",
        "    \"Probability of `word`.\"\n",
        "    return words_dict[word] / N\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in words_dict)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    \n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    \n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def norvig_correction(sentence):\n",
        "    \"Most probable spelling correction for words in input sentence.\"\n",
        "    corrected = []\n",
        "    for word in sentence: \n",
        "        corrected.append(correction(word))\n",
        "    return corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['nothing', 'went', 'long']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test norvig's approach on example \"Something went wrong\"\n",
        "norvig_correction('Smthing wrnt wong'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### My solution based on N-grams:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('abandoned',\n",
              " defaultdict(int,\n",
              "             {'building': 0.014885382554331646,\n",
              "              'buildings': 0.023816612086930634,\n",
              "              'car': 0.007889252753795772,\n",
              "              'cars': 0.010866329264662102,\n",
              "              'child': 0.004316760940756177,\n",
              "              'children': 0.01220601369455195,\n",
              "              'homes': 0.004019053289669544,\n",
              "              'house': 0.011759452217922,\n",
              "              'houses': 0.008931229532598988,\n",
              "              'mine': 0.010270913962488836,\n",
              "              'mines': 0.008038106579339089,\n",
              "              'railroad': 0.00506103006847276,\n",
              "              'a': 0.01726704376302471,\n",
              "              'after': 0.01012206013694552,\n",
              "              'all': 0.008186960404882405,\n",
              "              'and': 0.04257219410538851,\n",
              "              'any': 0.007293837451622507,\n",
              "              'as': 0.016522774635308126,\n",
              "              'at': 0.012057159869008634,\n",
              "              'babies': 0.004019053289669544,\n",
              "              'because': 0.009377791009228937,\n",
              "              'by': 0.10107174754391188,\n",
              "              'factories': 0.0034236379874962785,\n",
              "              'factory': 0.0037213456385829114,\n",
              "              'farm': 0.004465614766299494,\n",
              "              'for': 0.01845787436737124,\n",
              "              'her': 0.041530217326585295,\n",
              "              'him': 0.025454004167907114,\n",
              "              'his': 0.052396546591247393,\n",
              "              'hope': 0.004019053289669544,\n",
              "              'in': 0.06073236082167312,\n",
              "              'it': 0.022030366180410838,\n",
              "              'its': 0.02485858886573385,\n",
              "              'lot': 0.0034236379874962785,\n",
              "              'me': 0.010717475439118785,\n",
              "              'my': 0.006103006847275975,\n",
              "              'on': 0.013694551949985114,\n",
              "              'or': 0.018904435844001193,\n",
              "              'our': 0.00506103006847276,\n",
              "              'plans': 0.003572491813039595,\n",
              "              'property': 0.004763322417386127,\n",
              "              'ship': 0.004912176242929443,\n",
              "              'that': 0.010568621613575468,\n",
              "              'the': 0.1726704376302471,\n",
              "              'their': 0.048377493301577854,\n",
              "              'them': 0.024263173563560583,\n",
              "              'this': 0.008335814230425722,\n",
              "              'to': 0.016671628460851445,\n",
              "              'us': 0.006698422149449241,\n",
              "              'vehicles': 0.0037213456385829114,\n",
              "              'warehouse': 0.004167907115212861,\n",
              "              'warehouses': 0.003572491813039595,\n",
              "              'when': 0.00848466805596904,\n",
              "              'with': 0.004316760940756177,\n",
              "              'you': 0.005358737719559393}))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read bigrams with their corresponding frequencies from files\n",
        "BIGRAMS_FILE_1 = 'data/coca_all_links.txt'\n",
        "BIGRAMS_FILE_2 = 'data/bigrams.txt'\n",
        "\n",
        "bigram_model = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "with open(BIGRAMS_FILE_1, 'r', encoding='latin1') as file:\n",
        "    next(file)  # Skip the first line\n",
        "    for line in file:\n",
        "        freq, w1, w2, _, _ = line.strip().split()\n",
        "        bigram_model[w1][w2] = int(freq)\n",
        "\n",
        "with open(BIGRAMS_FILE_2, 'r', encoding='latin1') as file:\n",
        "    for line in file:\n",
        "        freq, w1, w2 = line.strip().split()\n",
        "        bigram_model[w1][w2] = int(freq)\n",
        "\n",
        "# Also get bigrams from the corpus file 'big.txt' using nltk bigrams\n",
        "for sentence in nltk.sent_tokenize(open(CORPUS_FILE).read()):\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
        "        bigram_model[w1][w2] += 1\n",
        "\n",
        "\n",
        "# Transform the frequencies into probabilities\n",
        "for w1 in bigram_model:\n",
        "    total_freq = float(sum(bigram_model[w1].values()))\n",
        "    for w2 in bigram_model[w1]:\n",
        "        bigram_model[w1][w2] /= total_freq\n",
        "\n",
        "\n",
        "\n",
        "next(iter(bigram_model.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('a', 'babe'), defaultdict(int, {'in': 1.0}))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get trigrams with their corresponding frequencies from file with fivegrams\n",
        "FIVEGRAMS_FILE = 'data/fivegrams.txt'\n",
        "\n",
        "trigram_model = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "with open(FIVEGRAMS_FILE, 'r', encoding='latin1') as file:\n",
        "    for line in file:\n",
        "        freq, w1, w2, w3, w4, w5 = line.strip().split()\n",
        "        if (w1, w2) not in trigram_model.items():\n",
        "            trigram_model[(w1, w2)][w3] = int(freq)\n",
        "        else:\n",
        "             trigram_model[(w1, w2)][w3] += int(freq)\n",
        "        \n",
        "        if (w2, w3) not in trigram_model.items():\n",
        "            trigram_model[(w2, w3)][w4] = int(freq)\n",
        "        else:\n",
        "             trigram_model[(w2, w3)][w4] += int(freq)\n",
        "             \n",
        "        if (w3, w4) not in trigram_model.items():\n",
        "            trigram_model[(w3, w4)][w5] = int(freq)\n",
        "        else:\n",
        "             trigram_model[(w3, w4)][w5] += int(freq)\n",
        "\n",
        "\n",
        "# Also get trigrams from the corpus file 'big.txt' using nltk trigrams\n",
        "for sentence in  nltk.sent_tokenize(open(CORPUS_FILE).read()):\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        trigram_model[(w1, w2)][w3] += 1\n",
        "\n",
        "\n",
        "    \n",
        "# Transform the frequencies into probabilities\n",
        "for w1w2 in trigram_model:\n",
        "    total_freq = float(sum(trigram_model[w1w2].values()))\n",
        "    for w3 in trigram_model[w1w2]:\n",
        "        trigram_model[w1w2][w3] /= total_freq\n",
        "\n",
        "next(iter(trigram_model.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bigram_correction(sentence, N=2):\n",
        "    \"Most probable spelling correction for sentence using bigram model.\"\n",
        "    sentence = [word.lower() for word in sentence]\n",
        "    corrected_sentence = [correction(sentence[0])]\n",
        "\n",
        "    for i in range(len(sentence) - N + 1):\n",
        "        max_P = 0\n",
        "        best_candidate = sentence[i + N - 1]\n",
        "\n",
        "        for candidate in list(candidates(sentence[i + N - 1])):\n",
        "            if bigram_model[sentence[i]][candidate] >= max_P:\n",
        "                max_P = bigram_model[sentence[i]][candidate]\n",
        "                best_candidate = candidate\n",
        "    \n",
        "        corrected_sentence.append(best_candidate)\n",
        "\n",
        "    return corrected_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['something', 'writ', 'gong']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_correction('Smthing wrnt wong'.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trigram_correction(sentence, N=3):\n",
        "    \"Most probable spelling correction for sentence using trigram model.\"\n",
        "    sentence = [word.lower() for word in sentence]\n",
        "    corrected_sentence = []\n",
        "    for word in bigram_correction(sentence[:N - 1]):\n",
        "        corrected_sentence.append(word)\n",
        "\n",
        "    for i in range(len(sentence) - N + 1):\n",
        "        max_P = 0\n",
        "        best_candidate = sentence[i + N - 1]\n",
        "\n",
        "        for candidate in list(candidates(sentence[i + 2])):\n",
        "            if trigram_model[(sentence[i], sentence[i + 1])][candidate] >= max_P:\n",
        "                max_P = trigram_model[(sentence[i], sentence[i + 1])][candidate]\n",
        "                best_candidate = candidate\n",
        "        \n",
        "        corrected_sentence.append(best_candidate)\n",
        "\n",
        "    return corrected_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['something', 'writ', 'gong']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trigram_correction('Smthing wrnt wong'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "To implement a context-sensitive spelling corrector, I decided to use N-gram language models, specifically bigrams and trigrams models that take into account the near context in the sentence. For the N-gram datasets, I used the data provided in Moodle. For the bigram model, I used the files `bigrams.txt` and `coca_all_links.txt`, and for trigrams `fivegrams.txt`, from which I  retrieved the trigrams. Furthermore, I created both bigrams and trigrams using `nltk` from the `big.txt` corpus file. All used data can be found in the `data` folder.\n",
        "\n",
        "After reading the data, I created the bigram and trigram models. These models contain probabilities for each bigram and trigram based on the datasets used. In terms of the spelling correction, I implemented the `bigram_correction` function for the bigram model and the `trigram_correction` function for the trigram model. The `bigram_correction` function iterates over the given sentence and computes the best candidates for each word using the bigram model. The `trigram_correction` function applies the `bigram_correction` function to the first two words of the sentence and then iterates over the rest of the sentence, computing the best candidates using the trigrams models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def corrupt_sentence(sentence, prob=0.5, mistake_type_prob=0.9):\n",
        "    \"Corrupt sentences from the test set by generating errors in words randomly.\"\n",
        "    sent_tokens = re.findall(r'\\w+', sentence)\n",
        "    curr_idx = 0\n",
        "    for token in sent_tokens:\n",
        "        correction = token\n",
        "        if random.random() < prob and not token.isnumeric():\n",
        "            if random.random() < mistake_type_prob:\n",
        "                correction = random.choice(list(edits1(token)))\n",
        "            else: \n",
        "                correction = random.choice(list(edits2(token)))\n",
        "            if correction == '':\n",
        "                correction = token\n",
        "        if token[0].isupper():\n",
        "            correction = correction[0].upper() + correction[1:]\n",
        "        idx = sentence.find(token, curr_idx)\n",
        "        sentence = sentence[:idx] + correction + sentence[idx + len(token):]\n",
        "        curr_idx = idx + len(correction)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period .\n",
            "TShe was the youngtest on tyhe two xaughters of ea most affectionate , indulgent fakher ; and jhad , in consequezce of gher sister ' s mawrriage , veen misstress lf uis house mrom a very esrly permiod .\n",
            "\n",
            "Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses ; and her place had been supplied by an excellent woman as governess , who had fallen little short of a mother in affection .\n",
            "Her mothecr had aied too long ajgoi for heyr to haie mordpe thal any gndisxinct rimembrance oaf her cargesses ; and her lace had been supplged by ib vexcellent woman gs governqess , bho haad fallee little shorx of a mothkr zin afpfection .\n",
            "\n",
            "Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both daughters , but particularly of Emma .\n",
            "Jixteen yeaps yhad Miss Txaylor ben in Mr . Woodhxouse ' zs fwmily , less ash a governess lhan as friend , vmery fond of both daughzers , but particularly of Emmad .\n",
            "\n",
            "Between _them_ it was more the intimacy of sisters .\n",
            "Betwetn _them_y it nas morep thae intuimacdy of sisters .\n",
            "\n",
            "Even before Miss Taylor had ceased to hold the nominal office of governess , the mildness of her temper had hardly allowed her to impose any restraint ; and the shadow of authority being now long passed away , they had been living together as friend and friend very mutually attached , and Emma doing just what she liked ; highly esteeming Miss Taylor ' s judgment , but directed chiefly by her own .\n",
            "Even befobe Miass Tcylor had chased o hold the nomilal office od governess , the amyildness sf hzr temper zhad hardly allowed her sto impose any restraint ; andi the shadow of authority yeing noz long passed qaway , they haid beend living together vas frhiend andh friend very mutually attached , aqnd MEma doing just whrat shwe liked ; highly esteeming Miss Taylor ' s judgment , btut dirzected chiefly byh hber owns .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use first 1500 meaningful sentences from Emma by Jane Austen from the Gutenberg electronic text archive (NLTK)\n",
        "# https://www.nltk.org/book/ch02.html\n",
        "\n",
        "emma_sentences = gutenberg.sents('austen-emma.txt')[3:1503]\n",
        "correct_test_set, corrupted_test_set = [], []\n",
        "\n",
        "# Create correct and generated corrupted test sets \n",
        "for sentence in emma_sentences[1:]:\n",
        "    sentence = \" \".join(sentence)\n",
        "    correct_test_set.append(sentence)\n",
        "    corrupted_test_set.append(corrupt_sentence(sentence))\n",
        "  \n",
        "for correct_words, corrupted in zip(correct_test_set[:5], corrupted_test_set[:5]):\n",
        "    print(correct_words)\n",
        "    print(corrupted)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "def eval_result(correct_sentence, result):\n",
        "    \"Evaluate resulted sentence comparing each word with corresponding word in correct sentence.\"\n",
        "    correct_words = 0\n",
        "    for i in range(len(correct_sentence)):\n",
        "        if correct_sentence[i].lower() == result[i].lower():\n",
        "            correct_words += 1\n",
        "    \n",
        "    return correct_words\n",
        "\n",
        "\n",
        "def eval_approaches(correct_input, corrupted_input):\n",
        "    \"Evaluate approaches and compute accuracy for each.\"\n",
        "    acc_Norvig, acc_Bigram, acc_Trigram = 0, 0, 0\n",
        "    total_words = 0\n",
        "\n",
        "    for correct, corrupted in zip(correct_input, corrupted_input):\n",
        "        result_Norvig = norvig_correction(corrupted.split())\n",
        "        result_Bigram = bigram_correction(corrupted.split())\n",
        "        result_Trigram = trigram_correction(corrupted.split())\n",
        "        \n",
        "        acc_Norvig += eval_result(correct.split(), result_Norvig)\n",
        "        acc_Bigram += eval_result(correct.split(), result_Bigram)\n",
        "        acc_Trigram += eval_result(correct.split(), result_Trigram)\n",
        "\n",
        "        total_words += len(correct.split())\n",
        "\n",
        "    acc_Norvig /= total_words\n",
        "    acc_Bigram /= total_words\n",
        "    acc_Trigram /= total_words\n",
        "    print(f\"Norvig's spelling corrector accuracy:\\t{acc_Norvig:.3f}\\nContext Bigram corrector accuracy:\\t{acc_Bigram:.3f}\\nContext Trigram corrector accuracy:\\t{acc_Trigram:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig's spelling corrector accuracy:\t0.703\n",
            "Context Bigram corrector accuracy:\t0.669\n",
            "Context Trigram corrector accuracy:\t0.627\n"
          ]
        }
      ],
      "source": [
        "# Compare approaches by accuracy\n",
        "eval_approaches(correct_test_set, corrupted_test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, I did not improve the accuracy of Norvig's approach. Maybe more strong or complex solutions, bigger N or not necessarily N-gram approach, might be considered."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
